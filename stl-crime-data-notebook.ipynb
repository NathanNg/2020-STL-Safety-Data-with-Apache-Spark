{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Exploring the City of Saint Louis public safety data with Apache Spark 2.3"]},{"cell_type":"markdown","metadata":{},"source":["The Saint Louis OpenData project contains hundreds of datasets for the city of Saint Louis. Open government data has the potential to increase the quality of life for residents, create more efficient government services, better public decisions, and even create new local businesses and services."]},{"cell_type":"markdown","metadata":{},"source":["![Gateway Arch](https://images.unsplash.com/photo-1514893011-72dfa15bd29c?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=8c2d25b2fcf02c87b1e9022f43affd05&auto=format&fit=crop&w=1491&q=80)"]},{"cell_type":"markdown","metadata":{},"source":["We start by mounting the Amazon S3 storage to the notebook."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["ACCESS_KEY = \"XXXXXXXXXXXXXXX\"\n","SECRET_KEY = \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n","ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n","AWS_BUCKET_NAME = \"xxxxx-xxxx\"\n","MOUNT_NAME = \"input\"\n","\n","dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n","display(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"]},{"cell_type":"markdown","metadata":{},"source":["List all mounted points"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["display(dbutils.fs.mounts())"]},{"cell_type":"markdown","metadata":{},"source":["The 2020 safety data of Saint Louis city is uploaded to S3 in csv format and pulled down to Databricks for analysis. You can list the file with the `%fs ls` command"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["%fs ls /mnt/input/input"]},{"cell_type":"markdown","metadata":{},"source":["Note, I combined 12 csv files into 1 file to get a full year of data.  I downloaded directly from this link: http://www.slmpd.org/Crimereports.shtml"]},{"cell_type":"markdown","metadata":{},"source":["I'm using \"spark\" as an entry point into all functionality in Spark 2.3."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["spark"]},{"cell_type":"markdown","metadata":{},"source":["Using the SparkSession, create a DataFrame from the CSV file by inferring the schema."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["crimeDataDF = spark.read.csv('/mnt/input/input/2020stlcrimedata.csv', header=True, inferSchema=True)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["crimeDataDF.count()"]},{"cell_type":"markdown","metadata":{},"source":["Display the data using a display function by Databricks."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["display(crimeDataDF)"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the above cell takes ~2 seconds to run b/c it is inferring the schema by sampling the file and reading through it.\n","\n","Inferring the schema works for ad-hoc analysis against smaller datasets. But when working on terabyte of data, it's better to provide an **explicit pre-defined schema manually**, so there's no inferring cost:"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DecimalType"]},{"cell_type":"markdown","metadata":{},"source":["Note that we are removing all space characters from the col names to prevent errors when writing to Parquet later"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["crimeSchema = StructType(\n","                     [\n","                     StructField('Complaint', StringType(), True),\n","                     StructField('CodedMonth', StringType(), True),\n","                     StructField('DateOccur', StringType(), True),\n","                     StructField('FlagCrime', StringType(), True),                  \n","                     StructField('FlagUnfounded', StringType(), True),       \n","                     StructField('FlagAdministrative', StringType(), True),       \n","                     StructField('Count', IntegerType(), True),       \n","                     StructField('FlagCleanup', StringType(), True),       \n","                     StructField('Crime', IntegerType(), True),       \n","                     StructField('District', IntegerType(), True),       \n","                     StructField('Description', StringType(), True),       \n","                     StructField('ILEADSAddress', IntegerType(), True),                  \n","                     StructField('ILEADSStreet', StringType(), True),       \n","                     StructField('Neighborhood', IntegerType(), True),       \n","                     StructField('LocationName', StringType(), True),       \n","                     StructField('LocationComment', StringType(), True),       \n","                     StructField('CADAddress', IntegerType(), True),       \n","                     StructField('CADStreet', StringType(), True),       \n","                     StructField('XCoord', StringType(), True),                 \n","                     StructField('YCoord', StringType(), True)\n","                     ]\n","                     )"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["crimeDataSDF = spark.read.csv('/mnt/input/input/2020stlcrimedata.csv', header=True, schema=crimeSchema)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["display(crimeDataSDF)"]},{"cell_type":"markdown","metadata":{},"source":["The csv file contains null records so we drop all null records from the table."]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["crimeDataSDF.na.drop(subset=\"Complaint\")"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["crimeDataSDF.count()"]},{"cell_type":"markdown","metadata":{},"source":["Look at the first 5 records in the DataFrame:"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["crimeDataSDF.show(5)"]},{"cell_type":"markdown","metadata":{},"source":["Print just the column names in the DataFrame:"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["crimeDataSDF.columns"]},{"cell_type":"markdown","metadata":{},"source":["Count how many rows total there are in DataFrame (and see how long it takes to do a full scan from remote disk/S3):"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["crimeDataSDF.count()"]},{"cell_type":"markdown","metadata":{},"source":["There are over ~46 thousand rows in the DataFrame and it takes ~2 seconds to do a full read of it."]},{"cell_type":"markdown","metadata":{},"source":["###  **Analysis with PySpark DataFrames and Spark SQL API**"]},{"cell_type":"markdown","metadata":{},"source":["Create a temp view to use spark.sql"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[],"source":["crimeDataSDF.createOrReplaceTempView(\"crimesql\")"]},{"cell_type":"markdown","metadata":{},"source":["**Q-1) How many different types of calls were made to the Police Department?**"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[],"source":["crimeDataSDF.select('Description').distinct().show(35, False)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["display(sqlContext.sql(\"SELECT Description FROM crimesql GROUP BY description\"), limit=35)"]},{"cell_type":"markdown","metadata":{},"source":["The queries above show the different type of calls to the police department."]},{"cell_type":"markdown","metadata":{},"source":["**Q-2) How many incidents of each call type were there?**"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["display(crimeDataSDF.select('Description').groupBy('Description').count().orderBy('Count', ascending=False))"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[],"source":["display(sqlContext.sql(\"SELECT Description, count(*) as count FROM crimesql GROUP BY description ORDER BY count desc\"))"]},{"cell_type":"markdown","metadata":{},"source":["Seems like the Saint Louis City Police department is called for leaving crime scene far more than any other type. Note that the above command took about 3 seconds to execute. In an upcoming section, we'll cache the data into memory for up to 100x speed increases."]},{"cell_type":"markdown","metadata":{},"source":["**Q-4) What is the most dangerous month in Saint Louis city?**"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["display(crimeDataSDF.select('CodedMonth').groupBy('CodedMonth').count().orderBy('Count',ascending=False))"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["display(sqlContext.sql(\"select CodedMonth, count(*) as count from crimesql group by CodedMonth ORDER BY count DESC\"))"]},{"cell_type":"markdown","metadata":{},"source":["Seems like August 2020 is the most dangerous month in Saint Louis city and June comes in second."]},{"cell_type":"markdown","metadata":{},"source":["### ![Spark Logo Tiny](http://curriculum-release.s3-website-us-west-2.amazonaws.com/wiki-book/general/logo_spark_tiny.png) ** Doing Date/Time Analysis**"]},{"cell_type":"markdown","metadata":{},"source":["**Q-4) How many service calls were logged on July 4th?**"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the date or time columns (DateOccur) is currently being interpreted as strings, rather than date or time objects:"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["crimeDataSDF.printSchema()"]},{"cell_type":"markdown","metadata":{},"source":["Let's use the unix_timestamp() function to convert the string into a timestamp:\n","\n","https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.sql.html?highlight=spark#pyspark.sql.functions.from_unixtime"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["from pyspark.sql.functions import *"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["# Note that PySpark uses the Java Simple Date Format patterns\n","\n","from_pattern1 = 'MM/dd/yyyy'\n","to_pattern1 = 'MM/dd/yyyy'\n","\n","from_pattern2 = 'MM/dd/yyyy HH:mm'\n","to_pattern2 = 'MM/dd/yyyy HH:mm'\n","\n","\n","crimeDataSTsDF = crimeDataSDF \\\n","  .withColumn('DateOccurTS', unix_timestamp(crimeDataSDF['DateOccur'], from_pattern2).cast(\"timestamp\")) \\\n","  .drop('DateOccur')"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.printSchema()"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.createOrReplaceTempView(\"crimesql_ts\")"]},{"cell_type":"markdown","metadata":{},"source":["Notice that the formatting of the timestamps is now different:"]},{"cell_type":"markdown","metadata":{},"source":["Note that July 4th, is the 185th day of the year in 2020.\n","\n","Filter the DF down to just 2020 and days of year equal 185:"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.filter(year('DateOccurTs') == '2020').filter(dayofyear('DateOccurTs') == 185).groupBy(dayofyear('DateOccurTs')).count().orderBy(dayofyear('DateOccurTs')).show()"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["display(sqlContext.sql(\"select count(*) as count FROM crimesql_ts WHERE cast(DateOccurTs as date)='2020-07-04'\"))"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["They were 133 calls made on July 4, 2020."]},{"cell_type":"markdown","metadata":{},"source":["**Q-5) How many service calls were logged in the first week of January 2020?**"]},{"cell_type":"markdown","metadata":{},"source":["Note that we can narrow down to the 2020 year and look at the first 7 days."]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.filter(year('DateOccurTs') == '2020').filter(dayofyear('DateOccurTs') >= 1).filter(dayofyear('DateOccurTs') <= 7).groupBy(dayofyear('DateOccurTs')).count().orderBy(dayofyear('DateOccurTs')).show()"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[],"source":["display(sqlContext.sql(\"SELECT cast(DateOccurTs as date) as Date, count(*) as count FROM crimesql_ts Where cast(DateOccurTs as date) between '2020-01-01' and '2020-01-07' Group BY cast(DateOccurTs as date) ORDER BY Date \"))"]},{"cell_type":"markdown","metadata":{},"source":["Visualize the results in a bar graph:"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["display(crimeDataSTsDF.filter(year('DateOccurTs') == '2020').filter(dayofyear('DateOccurTs') >= 1).filter(dayofyear('DateOccurTs') <= 7).groupBy(dayofyear('DateOccurTs')).count().orderBy(dayofyear('DateOccurTs')))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### ![Spark Logo Tiny](https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg) ** Memory, Caching and write to Parquet**"]},{"cell_type":"markdown","metadata":{},"source":["The DataFrame is currently comprised of 2 partitions:"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.rdd.getNumPartitions()"]},{"cell_type":"markdown","metadata":{},"source":["the repartition to 3 so that the data is divided evenly among 3 slots on Databrick Community Edition."]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["crimeDataSTsDF.repartition(3).createOrReplaceTempView(\"crimeDataVIEW\");"]},{"cell_type":"code","execution_count":74,"metadata":{},"outputs":[],"source":["spark.catalog.cacheTable(\"crimeDataVIEW\")"]},{"cell_type":"code","execution_count":75,"metadata":{},"outputs":[],"source":["# Call .count() to materialize the cache\n","spark.table(\"crimeDataVIEW\").count()"]},{"cell_type":"code","execution_count":76,"metadata":{},"outputs":[],"source":["crimeDataDF = spark.table(\"crimeDataVIEW\")"]},{"cell_type":"markdown","metadata":{},"source":["Once the data is cached, the full table scan from Amazon S3 took 1/10 of a second verus 2 seconds as before."]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["crimeDataDF.count()"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["spark.catalog.isCached(\"crimeDataVIEW\")"]},{"cell_type":"markdown","metadata":{},"source":["The 3 partitions are now cached in memory to match the Databrick 3 slots."]},{"cell_type":"markdown","metadata":{},"source":["![6 Partitions](http://curriculum-release.s3-website-us-west-2.amazonaws.com/sf_open_data_meetup/df_6_parts.png)"]},{"cell_type":"markdown","metadata":{},"source":["We can check the Spark UI to see the 3 partitions in memory:"]},{"cell_type":"markdown","metadata":{},"source":["Now that our data has the correct date/time types for each column and it is correctly partitioned, let's write it down as a parquet file for future loading:"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["%fs ls /mnt/input/input"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["crimeDataDF.write.format('parquet').save('dbfs:/mnt/input/input/data')"]},{"cell_type":"markdown","metadata":{},"source":["Now the directory should contain 3 .gz compressed Parquet files (one for each partition):"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["%fs ls dbfs:/mnt/input/input/data"]},{"cell_type":"markdown","metadata":{},"source":["Here's how you can easily read the parquet file from S3 in the future:"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["tempDF = spark.read.parquet('dbfs:/mnt/input/input/data/')"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["display(tempDF.limit(2))"]},{"cell_type":"markdown","metadata":{},"source":["The possibilities are endless with these datasets."]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["This notebook was inspired by Sameer Farooqui at Databricks."]}],"metadata":{"language_info":{"name":"python"},"name":"stl-crime-data-notebook","notebookId":4495755652863752},"nbformat":4,"nbformat_minor":0}
